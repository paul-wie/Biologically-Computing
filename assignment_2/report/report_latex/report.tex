\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amssymb}
\author{Paul Wieland}
\title{INF3490 Mandatory Assignment 2:
	Multilayer Perceptron}
\date{Deadline: Tuesday, October 16th, 2018 23:59:00}
\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 	%INTRODUCTION
	\section{Introduction}
	\subsection{Task}									%task
	We will build a Multilayer Perceptron to steer a robotic prosthetic hand. There are 40 inputs of electromyographic signals that we will classify. \\
	There are 8 classification values corresponding to a different hand motion: \\
	\begin{center}
		\includegraphics[width=0.4\linewidth]{pictures/hand}
		\\
		Figure 1: Possible motions \footnote{http://folk.uio.no/kyrrehg/pf/papers/glette-ahs08.pdf}
		\\
	\end{center}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{pictures/mlp}
		\\
		Figure 2: Multilayer Perceptron for our problem \footnote{https://www.uio.no/studier/emner/matnat/ifi/INF3490/h18/assignments/assignment-2/assignment\_2.pdf}
		\\
	\end{center}
	We build a Multilayer Perceptron with 40 entry nodes, that means one node for each input. Then there is one hidden layer with a various number of hidden nodes. For classifying the input, there are 8 output nodes corresponding to the 8 hand motions. We only use one hidden layer to solve this problem.

	\subsection{Training Data}							%training data
	For each input vector:
	\begin{center}

		\begin{equation}
		input = [i_1,i_2,i_3,i_4,...,i_{40}], i_n \in  \mathbb{R},n \in [40]
		\end{equation}
	\end{center}
	we have a target output vector:
	\begin{center}
		\begin{equation}
		 output = [c_1,c_2,c_3,c_4,...,c_8], c_n \in \{0,1\}, n \in [8],\sum_{n=1}^{8} c_n  = 1 
		 \end{equation}  
	\end{center}
	That means, forwarding the input should result in the given target vector. 
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%		%IMPLEMENTATION
	\section{Implementation}
	The file \textit{mlp.py} contains the class mlp. There are 5 functions that i will explain in detail.
	\subsection{Initialization}								%initalization
	The function \textit{ \_\_init\_\_(self, inputs, targets, nhidden)} has three important parameter that we need to initialize the Multilayer Perceptron. \\
	As the input data is given as a vector, it is a good choice to create two 2D-Array for the two weight layers. As the parameters \textit{inputs} and \textit{targets} have the type \textit{$<$class 'numpy.ndarray'$>$}, it is a good idea to work only with numpy arrays. \\
	\subsubsection{Dimension of the weight matrix}			%Dimension of weight matrix
		\begin{itemize}
		\item weight\_matrix\_1: \\
		The input vector in (1) has of course a size of 40. But we need to add the \textit{bias\_value -1} that can be seen in Figure 2. That means: \\ 
		\begin{center}
			\begin{equation}
			 \textit{weight\_matrix\_1} \in \mathbb{R}^{41 \times nhidden} 
			\end{equation} 
			\\
			 $w_{i,j} \in weight\_matrix\_1, w_{i,j}$: weight between input node(i) and hidden node(j) 
		\end{center}
		\item weight\_matrix\_2: \\
		There are \textit{nhidden} hidden nodes and 8 exit nodes. So we also need to take into account the \textit{bias\_value -1}. That means: \\
		\begin{center}
			\begin{equation}
			\textit{weight\_matrix\_2} \in \mathbb{R}^{(nhidden+1) \times 8} 
			\end{equation}
			\\
			$w_{i,j} \in weight\_matrix\_1, w_{i,j}$: weight between hidden node(i) and output node(j)  
		\end{center}
	\end{itemize} 
	Both, \textit{weigh\_matrix\_1} and \textit{weigh\_matrix\_2}, will be initialized randomly with values in [-1,1].
	
	\subsection{Forward}								%forward
	The forward function takes one input vector and runs it on the network. At first, the input vector must be expanded. The reason for this is that we have to take into account the bias value:	
	\begin{center}
		\begin{equation}
		\textit{input} \in \mathbb{R}^{1\times 41} 
		\end{equation} 
	\end{center}
	\subsubsection{Forward Phase 1}
	Subsequently it is possible to compute the \textit{hidden\_values}:
	\begin{center}
		\begin{equation}
		\textit{hidden\_values} = [h_1,h_2,h_3,h_4,...,h_8]
		\end{equation} 
	\end{center}

	\begin{center}
		\begin{equation}
		h_i = \sum_{n=1}^{41} input[n] \times weight\_matrix\_1[n][i] 
		\end{equation} 
	\end{center}
	This operation is done by the function \textit{ vec\_matr\_mult()} (that function can be found in the file \textit{operations.py}) that is doing a vector matrix multiplication with two for-loops. \\
	\subsubsection{Activation Function}
	After that is the activation function applied to all hidden nodes:
	\begin{center}
		\begin{equation}
		h_{new, i} = \frac{1}{1 + \ exp(-\beta h_i)} 
		\end{equation} 
		\\ (\textit{apply\_sigmoid\_activation()} in \textit{operations.py})
	\end{center}
	\subsubsection{Forward Phase 2}
	The result \textit{hidden\_values} is also expanded by the bias -1: 
	\begin{center}
		\begin{equation}
		\textit{hidden\_activation} \in \mathbb{R}^{1\times (nhidden +1)} 
		\end{equation} 
	\end{center}
	 So the \textit{output} vector can be calculated easily by another vector matrix multiplication:
	  \begin{center}
	  	\begin{equation}
	  	\textit{output} = hidden\_activation \cdot weight\_matrix\_2 
	  	\end{equation} 
	  \end{center}
  	\subsubsection{Output Error}
  	The forward function returns also a vector that is a converted version of the output-vector. This \textit{output\_discrete} has a 1 where the output vector has its highest value, the rest of the vector is 0. So the output-error for each node can be calculated very easy:
  	 \begin{center}
  		\begin{equation}
  		\delta_o(\kappa) = (y_{\kappa}-t_{\kappa}),
  		\end{equation}
  		$ y_{\kappa} \in \textit{output\_discrete} $, \\
  		$ t_{\kappa} \in \textit{output}$, see (2), \\
  		$ \textit{output\_error} = [\delta_o(1),...,\delta_o(8)] $
  	\end{center}
  	This calculation results from a linear output activation function.
  	\subsection{Calculate Hidden-Error}								%hidden error 
	From the forward phase we have gained the \textit{output-error}(11) and the \textit{activation-values}(7),(8) of the hidden nodes. That means it is possible to calculate the errors of the hidden nodes. The error of the hidden layer is determined by the function \textit{calculate\_hidden\_error()}. It follows the formula: \\
	 \begin{center}
		\begin{equation}
		\delta_h(\zeta) = a_{\zeta}(1-a_{\zeta}) \sum_{k=1}^{N} w_{\zeta}\delta_o(k)
		\end{equation} 
	\end{center}
	Where:
	\begin{itemize}
		\item $ a_{\zeta} \in \textit{hidden\_activation} $, see (7), (8)
		\item $ w_{\zeta} \in \textit{weight\_matrix\_2}$, see (4)
		\item $ \delta_o(k) \ in \textit{output\_error} $, see (11)
	\end{itemize}
	So, $w_{\zeta}$ is the weight that connects the hidden node with activation $a_{\zeta}$ and the output node with output-error $\delta_o(k)$.
	\subsection{Train the network}				%%%%%%%%%%%%%%%%%train the network
	To train the network means to backpropagate the errors we that we have calculated in 2.2.4 and 2.3. So the weights of \textit{weight\_matrix\_1} and \textit{weight\_matrix\_2} will be adjusted by the following formula:
	\begin{center}
		\begin{equation}
		w_{ij} = w_{ij} - \eta \delta_j x_i
		\end{equation} 
	\end{center}
	Where:
	\begin{itemize}
		\item $w_{ij}$ is the weight that connects an input node(i) and an output node(j). Input and output means only the activation direction, not any specific layer.
		\item $x_i$ is the activation value of an input node. From the view of \textit{weight\_matrix\_1} is the input vector of the network the meant input. From the view of \textit{weight\_matrix\_2} is the hidden layer the input one.
		\item  $\delta_j$ is the error of the output node.
		\item $\eta$ is the learning rate of the network. 

	\end{itemize}   
	\subsection{Earlystopping}				%%%%%%%%%%%%%%%%%earlystopping
	The function \textit{earlystopping()} observes the learning of the network. It should avoid overfitting. That means that the network is adjusted too much to the training data so unknown data will be classified very bad.
	\begin{center}
		\includegraphics[width=0.9\linewidth]{"pictures/Untitled Diagram"}
	\end{center}
	  
\end{document}